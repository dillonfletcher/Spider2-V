Documentation Source:
airbyte.com/tutorials/orchestrate-data-ingestion-and-transformation-pipelines.html

Documentation Title:
Orchestrate data ingestion and transformation pipelines with Dagster | Airbyte

Documentation Content:
Once this source is created, we can hook it up to our LocalPostgres destination:</p><img/></div><h2>Orchestrate Airbyte data ingestion pipelines with Dagster</h2><div><p>Now that we have some Airbyte connections to work with, we can get back to Dagster.</p><p>In the first few lines of <a>slack_github_analytics.py</a>, you’ll see the following code:</p><code>from dagster_airbyte import airbyte_resource, airbyte_sync_op

# …

sync_github = airbyte_sync_op.configured(
    {"connection_id": "&lt;YOUR AIRBYTE CONNECTION ID&gt;"}, name="sync_github"
)
sync_slack = airbyte_sync_op.configured(
    {"connection_id": "&lt;YOUR AIRBYTE CONNECTION ID&gt;"}, name="sync_slack"
)
</code><p>Here, we define the first two operations (or “ops”, in Dagster) of our job. <a>Dagster’s Airbyte integration</a>offers a pre-built op that will, when configured with a particular connection id, kick off a sync of that connection and wait until it completes. We also give these ops names (“sync_github” and “sync_slack”) to help people looking at this job understand what they’re doing.</p><p>This is where you can substitute in the relevant connection ids for the connections you set up in the previous steps. A quick way to find the id for a given connection is to click on it in the Airbyte UI, and grab the last section of the URL, i.e.:</p><img/><p>Once you’ve entered the correct values in for the `connection_id` fields, the code is ready to be executed!



Documentation Source:
airbyte.com/tutorials/orchestrate-data-ingestion-and-transformation-pipelines.html

Documentation Title:
Orchestrate data ingestion and transformation pipelines with Dagster | Airbyte

Documentation Content:
For our first source, we’ll sync information about the commits to the Dagster Github repository. Full details on how to set up this source can be found <a>here</a>, but your configuration should look something like this:</p><img/><p>This just points you at the official Dagster github repository, and sets a date for the earliest commit we want to ingest (just so that the initial sync doesn’t take too long). This uses a Github personal access token for authentication, which can be generated by following the instructions <a>here</a>, if you don’t already have one.</p><p>Once the source is set up, we can create a connection to create a <a>data ingestion pipeline</a>to load data from Github to Postgres:</p><img/><p>The most important bits here are to set the “Sync frequency” (at the top) to “manual”, as Dagster will handle triggering runs for this connection, and to select only the “commits” table (we won’t need data from any of the other streams). We also prefix the tables with “github_” to make it easier to tell where our data is coming from, and set this stream to “incremental” to make subsequent syncs more efficient.</p><h3>Set up a Slack source</h3><p>Our second source is the public Dagster Slack, where we’ll ingest the messages sent to one of our public channels.</p><p>If you’re following along, you can just read from whatever Slack channels you have access to, and if you don’t have easy access to a Slack API token, feel free to skip this entirely and replace the `slack_github_analytics.py` file in the Dagster code you cloned with `github_analytics.py`.</p><p>Once again, full instructions for setting up this source and generating a token can be found in the <a>Airbyte docs</a>, but your configuration should end up looking like this:</p><img/><p>Just like with the Github connection, we set a start date of 2022-01-01.



Documentation Source:
airbyte.com/tutorials/orchestrate-data-ingestion-and-transformation-pipelines.html

Documentation Title:
Orchestrate data ingestion and transformation pipelines with Dagster | Airbyte

Documentation Content:
Dagster comes with a UI tool, Dagit, which can be used to view and run your jobs. Dagster allows you to write Python code to define data pipelines as a series of interdependent steps. To get you started, the source code used for this demo is available on Github:</p><code>git clone https://github.com/OwenKephart/airbyte_demo.git
</code><p>‍</p><p>Once you have the code, you can install all the required dependencies (including Dagster and Dagit) using pip.</p><code>cd airbyte_demopip install -e .
</code><p>‍</p><p>To make sure that everything is working properly, navigate to the directory you just cloned, and run:</p><code>dagit -f airbyte_demo/slack_github_analytics.py
</code><p>This will spin up a local Dagit instance in your browser, which should look something like:</p><img/><p>This contains a single data pipeline (called a “job” in Dagster), which is named “slack_github_analytics”. Already, you get a rich set of information about the steps in this job, the types of their inputs and outputs, and how they connect to each other.</p><p>However, if you try running this job right now, you’ll quickly hit an error, as we haven’t actually set up the Airbyte connections that this job is meant to update, so let’s do that now.</p></div><h2>Set up Airbyte</h2><div><p>You can run Airbyte to configure data replication jobs from applications like Github and Slack to databases like Postgres.



Documentation Source:
airbyte.com/tutorials/orchestrate-data-ingestion-and-transformation-pipelines.html

Documentation Title:
Orchestrate data ingestion and transformation pipelines with Dagster | Airbyte

Documentation Content:
You can easily install Airbyte with Docker Compose and access the UI locally at http://localhost:8000:</p><code>$ git clone https://github.com/airbytehq/airbyte.git$ cd airbyte$ docker-compose up
</code></div><div><div><h3>Should you build or buy your data pipelines?</h3><p>Download our free guide and discover the best approach for your needs, whether it's building your ELT solution in-house or opting for Airbyte Open Source or Airbyte Cloud.</p><div>Download now</div></div><img/></div><h2>Ingest data from Slack and Github to Postgres</h2><div><p>If you already have some Airbyte connections set up for other purposes, feel free to use those instead! All you need for this demo to work is the Airbyte connection ids. However, if you’re starting from scratch, or just want to follow along, read on.</p><h3>Set up a Postgres destination</h3><p>While in reality, you’d probably want to ingest data into a cloud data warehouse such as Snowflake, we’ll use Postgres for this demo so you can run it entirely on your laptop. You can use docker to quickly get an empty Postgres database running on your local machine:</p><code>docker pull postgresdocker run --name airbyte-demo -p 5432:5432 -e POSTGRES_PASSWORD=password123 -d postgres
</code><p>‍</p><p>This will create a Postgres database with a user named “postgres”, whose password is “password123”. Once this is up and running, you can set up an Airbyte destination for this local instance:</p><img/><p>Just hit “Set up connection” at the bottom of the page, and you’re good to go.</p><h3>Set up a Github source</h3><p>Now that we have a destination to load our data, we’ll set up some data sources. For our first source, we’ll sync information about the commits to the Dagster Github repository.



