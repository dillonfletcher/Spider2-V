Documentation Source:
docs.astronomer.io/learn/airflow-database.txt

Documentation Title:
Understanding the Airflow metadata database | Astronomer Documentation

Documentation Content:
environ
[
"USERNAME_AIRFLOW_INSTANCE"
]
password
=
os
.
environ
[
"PASSWORD_AIRFLOW_INSTANCE"
]
# specify which dag to delete
dag_id
=
"dag_to_delete"
# send the deletion request
req
=
requests
.
delete
(
f"
{
ENDPOINT_URL
}
/api/v1/dags/
{
dag_id
}
"
,
auth
=
(
user_name
,
password
)
)
# print the API response
print
(
req
.
text
)
Was this page helpful?
Yes
No
Sign up for Developer Updates
Get a summary of new Astro features once a month.
Submit
You can unsubscribe at any time.
By proceeding you agree to our
Privacy Policy
, our
Website Terms
and to receive emails from Astronomer.
Tags:
Database
SQL
Components
Edit this page
Previous
Executor
Next
Scaling Airflow
Assumed knowledge
Database specifications
Metadata database content
User information (security)
DAG Configurations and Variables (Admin)
DAG and task runs (browse)
Other tables
Airflow metadata database best practices
Use the Airflow REST API to access the metadata database
Retrieve the number of successfully completed tasks
Pause and unpause a DAG
Delete a DAG
Legal
·
Privacy
·
Security
·
Cookie Preferences
© Astronomer 2023. Various trademarks held by their respective owners.



Documentation Source:
docs.astronomer.io/learn/cleanup-dag-tutorial.txt

Documentation Title:
Clean up the Airflow metadata database using DAGs | Astronomer Documentation

Documentation Content:
The params let you specify:
What age of data to delete. Any data that was created before the specified time will be deleted. The default is to delete all data older than 90 days.
Whether to run the cleanup as a dry run, meaning that no data is deleted. The DAG will instead return the SQL that would be executed based on other parameters you have specified. The default is to run the deletion without a dry run.
Which tables to delete data from. The default is all tables.
Step 2: Run the DAG
​
In this step, run the DAG in a local Airflow environment to practice the workflow for cleaning metadata DB data. When completing this process in a production environment, you would complete this process only after your Airflow environment has been running for a while.
Run
astro dev start
in your Astro project to start Airflow, then open the Airflow UI at
localhost:8080
.
In the Airflow UI, run the
db_cleanup
DAG by clicking the play button, then click
Trigger DAG w/ Config
. Configure the following params:
dry_run
:
true
tables
:
all_tables
clean_before_timestamp
:
datetime.now(tz=UTC) - timedelta(days=90)
Click
Trigger
.
After the task completes, click
Graph
.
Click a task run.
Click
Instance Details
.
Click
Log
.
Check that the
airflow db cleanup
command completed successfully. Note that if you created a new Astro project for this tutorial, the run will not show much data to be deleted.
You can now use this DAG to periodically clean data from the Airflow metadata DB as needed.
Was this page helpful?
Yes
No
Sign up for Developer Updates
Get a summary of new Astro features once a month.
Submit
You can unsubscribe at any time.
By proceeding you agree to our
Privacy Policy
, our
Website Terms
and to receive emails from Astronomer.
Edit this page
Previous
Customize Extra Links
Next
Custom XCom backends
Warnings
Prerequisites
Step 1: Create your DAG
Step 2: Run the DAG
Legal
·
Privacy
·
Security
·
Cookie Preferences
© Astronomer 2023.



Documentation Source:
docs.astronomer.io/learn/airflow-ui.txt

Documentation Title:
An introduction to the Airflow UI | Astronomer Documentation

Documentation Content:
By default the logs of the task execution are shown, while the
Pre task execution logs
and the
Post task execution logs
are collapsed and can be expanded by clicking on the respective log item.
Cluster activity tab
​
The cluster activity tab shows aggregated metrics for the entire Airflow cluster. It includes live metrics, such as currently occupied slots in different
pools
, unpaused DAGs, and scheduler health.
It also includes historical metrics like the states of past DAG runs and task instances, as well as how each DAG run was triggered.
Datasets tab
​
The
Dataset
tab links to a page showing all datasets that have been produced in the Airflow environment, as well as all dependencies between datasets and DAGs in a graph.
Click a dataset to open the history of all updates to the dataset that were recorded in the Airflow environment. You can use the Play button to manually trigger an update to a Dataset.
Security tab
​
Astro does not support the Security tab
On Astro, role-based access control is managed at the platform level. As a result, the Security tab is not needed and is not available on Airflow deployments on Astro.
The
Security
tab links to multiple pages, including
List Users
and
List Roles
, that you can use to review and manage Airflow role-based access control (RBAC). For more information on working with RBAC, see
Security
.
If you are running Airflow on Astronomer, the Astronomer RBAC will extend into Airflow and take precedence. There is no need for you to use Airflow RBAC in addition to Astronomer RBAC. Astronomer RBAC can be managed from the Astronomer UI, so the
Security
tab might be less relevant for Astronomer users.
Browse tab
​
The
Browse
tab links to multiple pages that provide additional insight into and control over your DAG runs and task instances for all DAGs in one place.
The DAG runs and task instances pages are the easiest way to view and manipulate these objects in aggregate. If you need to re-run tasks in multiple DAG runs, you can do so from this page by selecting all relevant tasks and clearing their status.



Documentation Source:
docs.astronomer.io/astro/view-logs.txt

Documentation Title:
View Deployment logs | Astronomer Documentation

Documentation Content:
For example,
acti*
returns results that include
action
and
acting
. The string search does not include fuzzy matching, so misspelled strings or incomplete strings without a wildcard,
*
, return zero results.
Time range
: Filter the logs displayed based on time.
Log type
: Filter based on whether the log message is from a scheduler, worker, webserver, or trigger.
View Airflow component logs locally
​
To show logs for your Airflow scheduler, webserver, or triggerer locally, run the following Astro CLI command:
astro dev logs
After you run this command, the most recent logs for these components appear in your terminal window.
By default, running
astro dev logs
shows logs for all Airflow components. To see logs only for a specific component, add any of the following flags to your command:
--scheduler
--webserver
--triggerer
To continue monitoring logs, run
astro dev logs --follow
. The
--follow
flag ensures that the latest logs continue to appear in your terminal window. For more information about this command, see
CLI Command Reference
.
Airflow task logs
​
Airflow task logs can help you troubleshoot a specific task instance that failed or retried. Based on your preference, you can choose to use to access task logs in the Astro UI or the Airflow UI. Both provide filters, search, and download options for task logs and share other information about your DAG performance on the same page.
Task logs for Astro Deployments are retained for 90 days. The task log retention policy is not currently configurable.
You can also access local Airflow task logs in your local
Airflow UI
or
printed to the terminal
.
Airflow task log levels
​
Similar to the Airflow component log levels, task logs might also be associated with one of the following log levels, that you can search or filter with:
Error
Warn
Info
Debug
Critical
View task logs on the Astro UI
​
To access task logs from the Astro UI:
In the Astro UI, select a Workspace.
Click
DAGs
.
Click the DAG you want to view task logs for.
Click a task run in the DAG run grid.
Click the
Logs
tab to switch from
Graph
view.



