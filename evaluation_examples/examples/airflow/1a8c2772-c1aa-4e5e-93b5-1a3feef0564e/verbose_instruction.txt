I am using Astronomer to deploy Airflow
The verbose instruction is about feature engineering and prepare machine learning experiments with following detailed steps:
1. Click the VS Code editor on the left panel or dock for the dag file "feature_eng.py".
2. First, we create a task group to prepare the MLFlow experiment with following tasks.
3. We define task list_existing_experiments to get information about existing MLFlow experiments by establishing mlflow client hook with given "MLFLOW_CONN_ID" calling the MLFlow API endpoint "api/2.0/mlflow/experiments/search" with the request parameter "max_results" given.
''' append code according to where corresponding annotation instructed
mlflow_hook = MLflowClientHook(mlflow_conn_id=MLFLOW_CONN_ID)
existing_experiments_information = mlflow_hook.run(
    endpoint="api/2.0/mlflow/experiments/search",
    request_params={"max_results": max_results},
).json()
'''
4. We define task check_if_experiment_exists to heck if the specified experiment already exists by branching the task based on the result of the previous task. If the experiment exists, the task should return the task ID "experiment_exists", otherwise it should return the task ID "create_experiment".
'''append code according to where corresponding annotation instructed
if existing_experiments_information:
    existing_experiment_names = [
        experiment["name"]
        for experiment in existing_experiments_information["experiments"]
    ]
    if experiment_name in existing_experiment_names:
        return "prepare_mlflow_experiment.experiment_exists"
    else:
        return "prepare_mlflow_experiment.create_experiment"
else:
    return "prepare_mlflow_experiment.create_experiment"
'''
5. We define task create_experiment to create a new MLFlow experiment with a specified name and save artifacts to the specified S3 bucket. The task creates a new experiment by establishing mlflow client hook with given "MLFLOW_CONN_ID" calling the MLFlow API endpoint "api/2.0/mlflow/experiments/create" with the request parameters "experiment_name" and "artifact_location" given. If the experiment is created successfully, the task should return the experiment information.
'''append code according to where corresponding annotation instructed
mlflow_hook = MLflowClientHook(mlflow_conn_id=MLFLOW_CONN_ID)
new_experiment_information = mlflow_hook.run(
    endpoint="api/2.0/mlflow/experiments/create",
    request_params={
        "name": experiment_name,
        "artifact_location": f"s3://{artifact_bucket}/",
    },
).json()
'''
6. We define task experiment_exists as an empty task to indicate that the experiment already exists.
'''append code according to where corresponding annotation instructed
experiment_already_exists = EmptyOperator(task_id="experiment_exists")
'''
7. We define task get_current_experiment_id that triggers if none of the above tasks are failed.It gets the ID of the specified MLFlow experiment by establishing mlflow client hook with given "MLFLOW_CONN_ID" calling the MLFlow API endpoint "api/2.0/mlflow/experiments/search" with the request parameter "max_results" given. The task should return the experiment ID of the specified experiment. The task should raise a ValueError if the experiment is not found in the MLFlow experiments.
'''append code according to where corresponding annotation instructed
mlflow_hook = MLflowClientHook(mlflow_conn_id=MLFLOW_CONN_ID)
experiments_information = mlflow_hook.run(
    endpoint="api/2.0/mlflow/experiments/search",
    request_params={"max_results": max_results},
).json()

for experiment in experiments_information["experiments"]:
    if experiment["name"] == experiment_name:
        return experiment["experiment_id"]

raise ValueError(f"{experiment_name} not found in MLFlow experiments.")
'''
8. We create workflow with following functions: First, it tries to get the experiment ID of the specified experiment with name "EXPERIMENT_NAME" and "MAX_RESULTS_MLFLOW_LIST_EXPERIMENTS" by calling the task "get_current_experiment_id". Then, it checks if the experiment already exists by calling the task "check_if_experiment_exists" with the parameters "experiment_name" and "existing_experiments_information" given. If the experiment exists, it triggers the task "experiment_exists". The task "create_experiment" is triggered if the experiment does not exist and creates a new experiment with the parameters "experiment_name" and "artifact_bucket" given. The workflow returns the experiment ID of the specified experiment.
'''append code according to where corresponding annotation instructed
(
    check_if_experiment_exists(
        experiment_name=EXPERIMENT_NAME,
        existing_experiments_information=list_existing_experiments(
            max_results=MAX_RESULTS_MLFLOW_LIST_EXPERIMENTS
        ),
    )
    >> [
        experiment_already_exists,
        create_experiment(
            experiment_name=EXPERIMENT_NAME,
            artifact_bucket=MLFLOW_ARTIFACT_BUCKET,
        ),
    ]
    >> experiment_id
)
'''
9. We implement code to start new MLFlow run. Set the experiment_id to the experiment_id passed into the function and the run_name to "Scaler". Use context manager (with statement) to automatically end the run when done.
'''append code according to where corresponding annotation instructed
with mlflow.start_run(experiment_id=experiment_id, run_name="Scaler") as run:
'''
10. We implement code to fit and transform X_encoded using the scaler object (an instance of StandardScaler), convert the result back into a DataFrame and set the column names to match those of X_encoded and assign this new DataFrame back to X_encoded inside mlflow run.
'''append code according to where corresponding annotation instructed
    X_encoded = pd.DataFrame(
        scaler.fit_transform(X_encoded), columns=X_encoded.columns
    )
'''
11. We implement code to log the scaler model to MLFlow with mlflow.sklearn.log_model() and set the artifact_path to "scaler".
'''append code according to where corresponding annotation instructed
    mlflow.sklearn.log_model(scaler, artifact_path="scaler")
'''
12. We implement code to log the mean of the scaler (which represents the mean of each feature after scaling) as a metric in MLFlow. To do this, first convert the scaler.mean_ to a DataFrame, set the index to the columns of X_encoded, and select the first column. Convert this to a dictionary and pass it to mlflow.log_metrics().
'''append code according to where corresponding annotation instructed
    mlflow.log_metrics(
        pd.DataFrame(scaler.mean_, index=X_encoded.columns)[0].to_dict()
    )
'''
13. At last, we define the workflow with tasks given in the file and defined above.
'''append code according to where corresponding annotation instructed
(
    start
    >> create_buckets_if_not_exists
    >> prepare_mlflow_experiment()
    >> build_features(
        raw_df=extracted_df,
        experiment_id="{{ ti.xcom_pull(task_ids='prepare_mlflow_experiment.get_current_experiment_id') }}",
        target_column=TARGET_COLUMN,
        categorical_columns=CATEGORICAL_COLUMNS,
        numeric_columns=NUMERIC_COLUMNS,
    )
    >> end
)
'''
14. Save the file content and switch to the opened ‘/home/user/projects/mlflow’ terminal.
15. In the terminal, type in the following command to restart the UI:
`astro dev restart` 
16. Click the Chromium on the left pannel
17. On the Airflow web page, find "feature_eng" in the DAG list and click the slider to the left of the name to Unpause dag; 
18. Click the triangle under the Action column on the far right of the row to trigger the dag; 
19. Wait until the status of all tasks in the 'Runs' column to change to "success" or "failed", we shall see the first run is failed;
20. Click the triangle just as step 18 instructed.
21. Wait until the status of all tasks in the 'Runs' column to change to "success" or "failed"