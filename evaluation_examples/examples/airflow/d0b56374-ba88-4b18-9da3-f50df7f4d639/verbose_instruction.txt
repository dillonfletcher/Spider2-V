I am using Astronomer to deploy Airflow
The verbose instruction is about data prediction for the machine learning experiments with following detailed steps:
1. Click the VS Code editor on the left panel or dock for the dag file "predict.py".
2. We implement code to define an Airflow task fetch_feature_df_no_target that fetches feature data from a previous feature_eng DAG, drops any missing values, and drops the target column. It returns a NumPy array. The name of the target column is TARGET_COLUMN.
‘’‘append code according to where corresponding annotation instructed
feature_df = context["ti"].xcom_pull(
    dag_id="feature_eng", task_ids="build_features", include_prior_dates=True
)
feature_df.dropna(inplace=True)
feature_df.drop(target_column, axis=1, inplace=True)
'''
3. We implement code to define an Airflow task fetch_model_run_id that fetches the model run ID from a previous train DAG with dag_id="train", task_ids="train_model", include_prior_dates=True.
'''append code according to where corresponding annotation instructed
model_run_id = context["ti"].xcom_pull(
    dag_id="train", task_ids="train_model", include_prior_dates=True
)
'''
4. We implement code to define an Airflow task add_line_to_file that uses S3Hook to read the requirements.txt file of the model from S3, adds two lines for boto3 and pandas, and then uploads the updated contents back to S3. The AWS connection ID is AWS_CONN_ID and the name of the S3 bucket is MLFLOW_ARTIFACT_BUCKET.
'''append code according to where corresponding annotation instructed
s3_hook = S3Hook(aws_conn_id=AWS_CONN_ID)
file_contents = s3_hook.read_key(
    key=context["ti"].xcom_pull(task_ids="fetch_model_run_id")
    + "/artifacts/model/requirements.txt",
    bucket_name=MLFLOW_ARTIFACT_BUCKET,
)
updated_contents = file_contents + "\nboto3" + "\npandas"
s3_hook.load_string(
    updated_contents,
    key=context["ti"].xcom_pull(task_ids="fetch_model_run_id")
    + "/artifacts/model/requirements.txt",
    bucket_name=MLFLOW_ARTIFACT_BUCKET,
    replace=True,
)
'''
5. We implement code to create an instance of ModelLoadAndPredictOperator named run_prediction that loads the model and runs the prediction. The URI of the model is s3://{MLFLOW_ARTIFACT_BUCKET}/{{ ti.xcom_pull(task_ids='fetch_model_run_id')}}/artifacts/model and the data is fetched_feature_df.
'''append code according to where corresponding annotation instructed
run_prediction = ModelLoadAndPredictOperator(
    mlflow_conn_id="mlflow_default",
    task_id="run_prediction",
    model_uri=f"s3://{MLFLOW_ARTIFACT_BUCKET}/"
    + "{{ ti.xcom_pull(task_ids='fetch_model_run_id')}}"
    + "/artifacts/model",
    data=fetched_feature_df,
)
'''
6. We implement code to define an Airflow task list_to_dataframe that converts the prediction results into a DataFrame.
'''append code according to where corresponding annotation instructed
df = pd.DataFrame(
    column_data, columns=["Predictions"], index=range(len(column_data))
)
'''
7. We implement code to use the aql.export_file function to save the prediction results to S3. The path of the output file is os.path.join("s3://", BUCKET_NAME, FILE_TO_SAVE_PREDICTIONS).
'''append code according to where corresponding annotation instructed
pred_file = aql.export_file(
    task_id="save_predictions",
    input_data=prediction_data,
    output_file=File(os.path.join("s3://", BUCKET_NAME, FILE_TO_SAVE_PREDICTIONS)),
    if_exists="replace",
)
'''
8. Save the file content and switch to the opened ‘/home/user/projects/mlflow’ terminal.
9. In the terminal, type in the following command to restart the UI:
`astro dev restart` 
10. Click the Chromium on the left pannel
11. On the Airflow web page, find "feature_eng" in the DAG list and click the slider to the left of the name to Unpause dag; 
12. On the Airflow web page, find "train" in the DAG list and click the slider to the left of the name to Unpause dag;
13. On the Airflow web page, find "predict" in the DAG list and click the slider to the left of the name to Unpause dag;  
14. Click the triangle under the Action column on the far right of the "feature_eng" row to trigger the dag and start the pipeline; 
15. Wait until the status of all tasks in the 'Runs' column to change to "success" or "failed", we shall see the first run is failed;
16. Click the triangle just as step 14 instructed.
17. Wait until the status of all tasks in the 'Runs' column to change to "success" or "failed", this may takes a while.